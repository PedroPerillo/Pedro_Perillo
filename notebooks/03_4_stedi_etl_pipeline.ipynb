{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85d4d38-1898-4b3e-a4d9-5222479c67e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load raw device and step test data"
    }
   },
   "outputs": [],
   "source": [
    "df_device = spark.table(\"workspace.bronze.device_message_raw\")\n",
    "df_steps = spark.table(\"workspace.bronze.rapid_step_test_raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe63a7b-3281-4e8d-b97f-7e833e7aa92e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display loaded device and step tables"
    }
   },
   "outputs": [],
   "source": [
    "display(df_device)\n",
    "display(df_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e956b9d-76dc-4b24-ade7-e3fa8f1b30de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract numeric distance_cm before join"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "df_device = df_device.withColumn(\n",
    "    \"distance_cm\", regexp_extract(col(\"distance\"), r\"(\\d+)\", 1).cast(\"int\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a073975-cf41-456d-bfaf-06eafdf3f10c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add source column to both DataFrames"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_device = df_device.withColumn(\"source\", lit(\"device\"))\n",
    "df_steps = df_steps.withColumn(\"source\", lit(\"step\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef59857-b85c-43cd-a029-2c67ed99b435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_steps_window = df_steps.select(\n",
    "\"device_id\", \"start_time\", \"stop_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe61ccb-3727-4149-b94d-d3e8b850348a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join and label device readings"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, regexp_extract, col\n",
    "\n",
    "df_labeled = (\n",
    "    df_device.alias(\"d\")\n",
    "    .join(\n",
    "        df_steps_window.alias(\"s\"),\n",
    "        (col(\"d.device_id\") == col(\"s.device_id\")) &\n",
    "        (col(\"d.timestamp\").between(col(\"s.start_time\"), col(\"s.stop_time\"))),\n",
    "        \"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"step_label\",\n",
    "        when(col(\"s.start_time\").isNotNull(), \"step\").otherwise(\"no_step\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"distance_cm\",\n",
    "        regexp_extract(col(\"d.distance\"), r\"(\\d+)\", 1).cast(\"int\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d818f426-2df1-4e0a-864f-36f9e78c92d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select and curate final DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_labeled.selectExpr(\n",
    "    \"timestamp\",\n",
    "    \"sensor_type as sensorType\",\n",
    "    \"distance_cm\",\n",
    "    \"d.device_id as deviceId\",\n",
    "    \"step_label\",\n",
    "    \"source\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b1c97e-f569-418a-a6f4-8e087d05ccc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write curated table to silver schema"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE silver\")\n",
    "df_final.write.mode(\"overwrite\").saveAsTable(\"labeled_step_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b2e9a6-afc5-4024-acce-2568e07f6489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  step_label,\n",
    "  COUNT(*) AS row_count\n",
    "FROM labeled_step_test\n",
    "GROUP BY step_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1326a5cc-7e5a-409a-879f-adbf330c53e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL: Check for unexpected step_label values"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM labeled_step_test\n",
    "WHERE step_label NOT IN ('step', 'no_step')\n",
    "OR step_label IS NULL\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a943fc9-88d0-42e0-9890-a284f8a05859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL: Check for unexpected source values"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM labeled_step_test\n",
    "WHERE source NOT IN ('device', 'step')\n",
    "OR source IS NULL\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e18b874-6dce-415b-8631-dcbd0ccdcaa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.createOrReplaceTempView(\"final_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094383fd-a1e9-4f68-b7ee-5ef24ae76e5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rewrite curated table with SQL"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE labeled_step_test AS\n",
    "SELECT * FROM final_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8eb14a7-4553-4804-a963-d071b0bd1b40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When automating health related data pipelines, engineers need to be careful with patient privacy and make sure sensitive data is properly secured and only accessible to the right people. Data accuracy is just as important, since bad or incomplete data can lead to misleading results or poor decisions. Itâ€™s also important to watch for bias in the data and avoid building systems that unfairly impact certain groups. Engineers should be clear that these pipelines are meant to support analysis, not provide medical diagnoses or treatment advice. In the end, ethical automation is about protecting people first and being thoughtful about how health data is collected, processed, and used.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6992421926570172,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_4_stedi_etl_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
